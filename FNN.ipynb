{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from typing import Tuple\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "seed = 47\n",
    "\n",
    "X, y = fetch_openml('CIFAR_10', as_frame=False, return_X_y=True)\n",
    "\n",
    "y_int = y.astype(int)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y_int, test_size=0.25, random_state=seed, shuffle=True)\n",
    "\n",
    "def scale_data(X_tr: np.array, X_te: np.array) -> tuple[np.array, np.array]:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_tr)\n",
    "    X_tr_scaled = scaler.transform(X_tr)\n",
    "    X_te_scaled = scaler.transform(X_te)\n",
    "    return X_tr_scaled, X_te_scaled\n",
    "\n",
    "X_tr_scaled, X_te_scaled = scale_data(X_tr, X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def errors_for_train_sizes_mlp(X_tr: np.array, y_tr: np.array, X_te: np.array, y_te: np.array, seed: int, train_sizes: list[int]) -> tuple[list, list, list, list]:\n",
    "    tr_err_mlp = []\n",
    "    te_err_mlp = []\n",
    "\n",
    "    mlp_params = {\n",
    "        'hidden_layer_sizes': (64,),\n",
    "        'activation': 'relu',\n",
    "        'solver': 'sgd',\n",
    "        'learning_rate_init': 0.001,\n",
    "        'batch_size': 100,\n",
    "        'random_state': seed,\n",
    "        'max_iter': 1500\n",
    "    }\n",
    "\n",
    "    for n_tr in train_sizes:\n",
    "        X_tr_subset = X_tr[:n_tr]\n",
    "        y_tr_subset = y_tr[:n_tr]\n",
    "        \n",
    "        mlp = MLPClassifier(**mlp_params)\n",
    "        mlp.fit(X_tr_subset, y_tr_subset)\n",
    "\n",
    "        y_tr_pred = mlp.predict(X_tr_subset)\n",
    "        tr_err = 1 - accuracy_score(y_tr_subset, y_tr_pred)\n",
    "        tr_err_mlp.append(tr_err)\n",
    "        \n",
    "        y_te_pred = mlp.predict(X_te)\n",
    "        te_err = 1 - accuracy_score(y_te, y_te_pred)\n",
    "        te_err_mlp.append(te_err)\n",
    "    return tr_err_mlp, te_err_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_for_train_sizes_lr(X_tr: np.array, y_tr: np.array, X_te: np.array, y_te: np.array, seed: int, train_sizes: list[int]) -> tuple[list, list, list, list]:    \n",
    "    tr_err_lr = []\n",
    "    te_err_lr = []\n",
    "\n",
    "    for n_tr in train_sizes:\n",
    "        X_tr_subset = X_tr[:n_tr]\n",
    "        y_tr_subset = y_tr[:n_tr]\n",
    "        \n",
    "        lr = LogisticRegression(random_state=seed)\n",
    "        lr.fit(X_tr_subset, y_tr_subset)\n",
    "        \n",
    "        y_tr_pred = lr.predict(X_tr_subset)\n",
    "        tr_err = 1 - accuracy_score(y_tr_subset, y_tr_pred)\n",
    "        tr_err_lr.append(tr_err)\n",
    "        \n",
    "        y_te_pred = lr.predict(X_te)\n",
    "        te_err = 1 - accuracy_score(y_te, y_te_pred)\n",
    "        te_err_lr.append(te_err)\n",
    "  \n",
    "    return tr_err_lr, te_err_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors_for_train_sizes_mlp_lr(tr_err_mlp: list, te_err_mlp: list, tr_err_lr: list, te_err_lr: list, train_sizes: list[int]) -> None:\n",
    "    plt.semilogx(train_sizes, tr_err_mlp, label='MLP Training Error', marker='x', color = \"orange\")\n",
    "    plt.semilogx(train_sizes, te_err_mlp, label='MLP Testing Error', marker='x', color = \"orange\", linestyle = \"--\")\n",
    "\n",
    "    plt.semilogx(train_sizes, tr_err_lr, label='Logistic Regression Training Error', marker='x', color = \"blue\")\n",
    "    plt.semilogx(train_sizes, te_err_lr, label='Logistic Regression Testing Error', marker='x', color = \"blue\", linestyle = \"--\")\n",
    "\n",
    "    plt.xlabel('Num. Training Data Points')\n",
    "    plt.ylabel('Error Rate')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cs178/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:609: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/cs178/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "train_sizes = [50, 500, 2000, 5000]\n",
    "tr_err_mlp, te_err_mlp = errors_for_train_sizes_mlp(X_tr_scaled, y_tr, X_te_scaled, y_te, seed, train_sizes)\n",
    "tr_err_lr, te_err_lr = errors_for_train_sizes_lr(X_tr_scaled, y_tr, X_te_scaled, y_te, seed, train_sizes)\n",
    "plot_errors_for_train_sizes_mlp_lr(tr_err_mlp, te_err_mlp, tr_err_lr, te_err_lr, train_sizes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
